---
title: Illustration of variational Gaussian approximation for Poisson-normal model with one unknown
author: Peter Carbonetto
output: workflowr::wflow_html
---

Here we demonstrate the variational Gaussian approximation for the
Poisson-normal in the simplest case when there is one unknown. Under
the data model, the counts $y_1, \ldots, y_n$ are Poisson with rates
$\lambda_1, \ldots, \lambda_n$, in which $\log \lambda_i = b_0 + x_i
b$. The unknown $b$ is assigned a normal prior with zero mean and
standard deviation $\sigma_0$. Here we use variational methods to
approximate the posterior of $b$ with a normal density $N(b; \mu,
s^2)$. See the [Overleaf document][overleaf] for a more detailed
description of the model and variational approximation.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the functions implementing the variational inference algorithms
and set the seed.

```{r setup}
source("../code/vgapois.R")
set.seed(1)
```

Simulate data
-------------

Simulate counts from the following Poisson model: $y_i \sim
\mathrm{Poisson}(\lambda_i)$, in which $\log \lambda_i = b_0 + b x_i$.

```{r sim-data}
n  <- 10
b0 <- -1
b  <- 1.5
x  <- rnorm(n)
r  <- exp(b0 + x*b)
y  <- rpois(n,r)
```

Compute Monte Carlo estimate of marginal likelihood
---------------------------------------------------

Here we compute an importance sampling estimate of the marginal
log-likelihood We will compare this against the lower bound to the
marginal likelihood obtained by the variational approximation.

```{r importance-sampling}
s0   <- 3
ns   <- 1e5
b    <- rnorm(ns,sd = sqrt(s0))
logw <- rep(0,ns)
for (i in 1:ns)
  logw[i] <- compute_loglik_pois(x,y,b0,b[i])
a    <- max(logw)
logZ <- log(mean(exp(logw - a))) + a
```

Fit variational approximation
-----------------------------

Fit the variational Gaussian approximation by optimizing the
variational lower bound (the "ELBO").

```{r fit-vga}
fit <- vgapois1(x,y,b0,s0)
cat(fit$message,"\n")
cat(sprintf("Monte Carlo estimate:    %0.12f\n",logZ))
cat(sprintf("Variational lower bound: %0.12f\n",-fit$value))
```

Here we see that the ELBO slightly undershoots the marginal likelihood.

Compare exact and approximate posterior distributions
-----------------------------------------------------

Plot the exact posterior density (dark blue), and compare it against the
variational Gaussian approximation (magenta).

```{r, fig.height=2.5, fig.width=4}
ns   <- 1000
b    <- seq(-1,3,length.out = ns)
logp <- rep(0,ns)
for (i in 1:ns)
  logp[i] <- compute_logp_pois(x,y,b0,b[i],s0)
par(mar = c(4,4,1,0))
plot(b,exp(logp - max(logp)),type = "l",lwd = 2,col = "darkblue",
     xlab = "b",ylab = "posterior")
mu <- fit$par["mu"]
s  <- fit$par["s"]
pv <- dnorm(b,mu,sqrt(s))
lines(b,pv/max(pv),col = "magenta",lwd = 2)
```

The true posterior is very much "bell shaped", so as expected the
normal approximation is a good fit to the true posterior.

[overleaf]: https://www.overleaf.com/read/fbwkmbcjzctc
