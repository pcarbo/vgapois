---
title: Illustration of variational Gaussian approximation for Poisson-normal model with one unknown
author: Peter Carbonetto
output: workflowr::wflow_html
---

Here we demonstrate the variational Gaussian approximation for the
Poisson-normal in the simplest case when there is one unknown. Under
the data model, the counts $y_1, \ldots, y_n$ are Poisson with
log-rates $\lambda_1, \ldots, \lambda_n$, in which $\lambda_i = a_i +
x_i b$. The unknown $b$ is assigned a normal prior with zero mean and
standard deviation $\sigma_0$. Here we use variational methods to
approximate the posterior of $b$ with a normal density $N(b; \mu,
s^2)$. See the [Overleaf document][overleaf] for a more detailed
description of the model and variational approximation.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the functions implementing the variational inference algorithms
and set the seed.

```{r setup}
source("../code/vgapois.R")
set.seed(1)
```

Simulate data
-------------

Simulate counts from the following Poisson model: $y_i \sim
\mathrm{Poisson}(e^{\lambda_i})$, in which $\lambda_i = a_i + b x_i$.

```{r sim-data}
n <- 10
b <- 1
a <- rnorm(n,mean = -2)
x <- rnorm(n)
r <- a + x*b
y <- rpois(n,exp(r))
```

Compute Monte Carlo estimate of marginal likelihood
---------------------------------------------------

Here we compute an importance sampling estimate of the marginal
log-likelihood We will compare this against the lower bound to the
marginal likelihood obtained by the variational approximation.

```{r importance-sampling-1}
s0   <- 3
ns   <- 1e5
b    <- rnorm(ns,sd = sqrt(s0))
logw <- rep(0,ns)
for (i in 1:ns)
  logw[i] <- compute_loglik_pois1(x,y,a,b[i])
d    <- max(logw)
logZ <- log(mean(exp(logw - d))) + d
```

Compute importance sampling estimates of the mean and variance.

```{r importance-sampling-2}
w     <- exp(logw - d)
w     <- w/sum(w)
mu.mc <- sum(w*b)
s.mc  <- sum(w*b^2) - mu.mc^2
```

Fit variational approximation
-----------------------------

Fit the variational Gaussian approximation by optimizing the
variational lower bound (the "ELBO").

```{r fit-vgapois-univariate}
fit1 <- vgapois1(x,y,a,s0)
mu   <- fit1$par["mu"]
s    <- fit1$par["s"]
cat(fit1$message,"\n")
cat(sprintf("Monte Carlo estimate:    %0.12f\n",logZ))
cat(sprintf("Variational lower bound: %0.12f\n",-fit1$value))
```

Here we see that the ELBO slightly undershoots the marginal likelihood.

Compare exact and approximate posterior distributions
-----------------------------------------------------

Compare the importance sampling and variational estimates of the mean
and standard deviation.

```{r compare-1}
cat(sprintf("Monte Carlo estimates: mean=%0.4f, sd=%0.4f\n",mu.mc,sqrt(s.mc)))
cat(sprintf("Variational estimates: mean=%0.4f, sd=%0.4f\n",mu,sqrt(s)))
```

Plot the exact posterior density (dark blue), and compare it against the
variational Gaussian approximation (magenta).

```{r compare-2, fig.height=2.5, fig.width=4}
ns   <- 1000
b    <- seq(-1,3,length.out = ns)
logp <- rep(0,ns)
for (i in 1:ns)
  logp[i] <- compute_logp_pois1(x,y,a,b[i],s0)
par(mar = c(4,4,1,0))
plot(b,exp(logp - max(logp)),type = "l",lwd = 2,col = "darkblue",
     xlab = "b",ylab = "posterior")
pv <- dnorm(b,mu,sqrt(s))
lines(b,pv/max(pv),col = "magenta",lwd = 2)
```

The true posterior is very much "bell shaped", so as expected the
normal approximation is a good fit to the true posterior.

Verify multivariate implementation
----------------------------------

The multivariate implementation of the variational Gaussiaan
approximation for the Poisson-normal model also handles the limiting
univariate case, and should return the same result as `vgapois1`.

```{r vgapois-multivariate}
fit <- vgapois(x,y,a,s0)
print(mu - fit$mu)
print(s - fit$S)
print(fit1$value - fit$value)
```

[overleaf]: https://www.overleaf.com/read/fbwkmbcjzctc
